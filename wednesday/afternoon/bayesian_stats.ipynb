{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc044ad5-1f58-4d6a-ba21-17c68d283d49",
   "metadata": {},
   "source": [
    "# Bayesian statistics with Jax/Numpyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c1a07-8e97-4339-b696-5c4f6a73e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "from numpyro.infer import MCMC, NUTS, Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1127e-69e8-4d56-8cbe-555e093e5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpyro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e27a1-eced-4a42-8232-07a4b6838058",
   "metadata": {},
   "source": [
    "## Bayesian refresher\n",
    "\n",
    "### Bayes law\n",
    "\n",
    "Bayes law is given by:\n",
    "\n",
    "$$P(A | B) = \\frac{P(B | A) P(A)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1632f36-9915-47f9-8620-522dc9c28da6",
   "metadata": {},
   "source": [
    "**Obligatory medical test example**\n",
    "\n",
    "Prior to discussing Bayes law in the context of statistical inference, we believe that it is productive to begin by examining a commonly used example for introducing Bayes law.\n",
    "\n",
    "Consider a disease that affects 0.1\\% of the population (i.e. 1 in 1,000 individuals actually have the disease). Suppose a country is screening individuals at an airport for whether they have this disease or not. They are administering a test with a sensitivity of 99\\% and a specificity of 90\\%. Now, suppose that an individual entering the country has tested\n",
    "positive for the disease, what is the probability that they have the disease?\n",
    "\n",
    "One of the surprising aspects of this problem is that many people \"intuitively\" believe that the probability the individual has the disease is relatively high. However, the actual probability that the individual has the disease is <1\\%.\n",
    "\n",
    "We can get this answer by applying Bayes law. The probability of interest ($P(A | B)$) is \"the probability that the individual has the disease given they tested positive for the disease\". So let $A$ be defined as \"individual has the disease\" and $B$ be defined as \"tested positive for the disease\".\n",
    "\n",
    "With these defined, we can start calculating pieces of Bayes law:\n",
    "\n",
    "* $P(B | A)$: This is the probability the test is positive given the individual does in fact have the disease. This is given by the sensitivity (aka 0.99).\n",
    "* $P(A)$: This is the probability of having the disease with no other information which is\n",
    "  0.001 since 0.1\\% of the population has the disease.\n",
    "* $P(B)$: This is the probability of getting a positive test which can be broken into two separate pieces -- You either test positive and have the disease or you test positive and do not have the disease (i.e. $P(B) = P(B | A) P(A) + P(B | ~A) P(~A)$). We already have\n",
    "  identified $P(B | A)$ and $P(A)$ above and we know that $P(~A) = 1 - P(A)$.\n",
    "  The specificity of the test gives us $P(~B | ~A)$ and $P(B | ~A) = 1 - P(~B | ~A)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769f729-f3ac-4bf6-affc-6a5a6c25bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bga = 0.99\n",
    "p_a = 0.001\n",
    "p_b = 0.99*p_a + 0.90*(1 - p_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ea375-6f4b-43c6-82fd-9e74fe5e1270",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bga*p_a / p_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99314a89-d642-4284-a343-187724f2a19c",
   "metadata": {},
   "source": [
    "## Bayesian statistics\n",
    "\n",
    "Like other types of statistics, Bayesian statistics is focused on the inverse problem:\n",
    "\n",
    "> Given some data, $Y$, and a parameterized class of models, $f(\\Theta)$, find the parameter vector $\\theta \\in \\Theta$ that could have produced the data.\n",
    "\n",
    "In Bayesian statistics we recover a \"joint distribution of parameters\" that could have generated the data. We use Bayes law as below to do this:\n",
    "\n",
    "$$\\underbrace{P(\\theta | Y)}_{\\text{posterior}} = \\frac{\\overbrace{P(Y | \\theta)}^{\\text{likelihood}} \\overbrace{P(\\theta)}^{\\text{prior}}}{\\underbrace{P(Y)}_{\\text{normalizing component}}}$$\n",
    "\n",
    "We have labeled each of these components according to names that are commonly used to describe\n",
    "them:\n",
    "\n",
    "\n",
    "**Normalizing component**: $P(Y)$\n",
    "\n",
    "The normalizing component is often ignored because it's simply the value needed to ensure that the posterior is a probability distribution -- In fact, you will often find Bayes law written as $P(\\theta | Y) \\propto P(Y | \\theta) P(\\theta)$\n",
    "\n",
    "\n",
    "**Likelihood**: $P(Y | \\theta)$\n",
    "\n",
    "We refer to this term as the likelihood and it establishes how likely it was to observe certain realizations o the data for a given parameter $\\theta$.\n",
    "\n",
    "\n",
    "**Prior**: $P(\\theta)$\n",
    "\n",
    "The prior is what most people would identify as the \"defining feature\" of Bayesian statistics. The\n",
    "prior specifies the \"prior belief\" that the statistician assigns to different parameter values\n",
    "_before_ having seen any data.\n",
    "\n",
    "_Why priors?_\n",
    "\n",
    "One often begins their inference process with some idea about what parameters make sense and which ones don't. A prior reflects the subjective beliefs of the statistician who is running the analysis (or the beliefs of the audience that they are trying to convince).\n",
    "\n",
    "For example, suppose you were modeling a demand curve. We have come to accept that typically demand curves slope downwards which is a belief that we could express with a prior.\n",
    "\n",
    "A natural question that follows the introduction of the idea of a prior is, \"doesn't a prior make your analysis subjective rather than strictly objective?\" Yes, it does. If you would like to understand why we don't think that this is a problem, we would reference\n",
    "\n",
    "Some great discusson on priors\n",
    "\n",
    "* https://stat.columbia.edu/~gelman/research/published/philosophy_chapter.pdf\n",
    "* https://statmodeling.stat.columbia.edu/2016/12/13/bayesian-statistics-whats/\n",
    "* https://projecteuclid.org/journals/bayesian-analysis/volume-3/issue-3#toc\n",
    "\n",
    "**Posterior**: $P(\\theta | Y)$\n",
    "\n",
    "The posterior is the conditional distribution that describes the statistician's beliefs about parameter values given the data that they have observed. All of Bayesian statistics will depend on being able to find (and sample from) the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30961a5d-9115-444d-882e-f02cef7a4e97",
   "metadata": {},
   "source": [
    "### \"Old school\" Bayesian stats\n",
    "\n",
    "Bayesian statistics used to mostly use ideas of a \"conjugate prior\" to derive the posterior.\n",
    "\n",
    "If the prior, $P(\\theta)$, and the posterior, $P(\\theta | Y)$, belong to the same probability distribution family for a specified likelihood, $P(Y | \\theta)$, then we say the prior is a conjugate prior for the likelihood.\n",
    "\n",
    "Conjugate priors are convenient because they allow us to have a closed-form expression for the posterior. We will examine a single example for today but you can reference the [Wikipedia conjugate priors table](wiki:Conjugate_prior#Table_of_conjugate_distributions) to find more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111a8e8-2bcf-4c1d-9334-c4c657a35d73",
   "metadata": {},
   "source": [
    "**Beta-Bernoulli**\n",
    "\n",
    "There is a known parameter $n$ which is the number of Bernoulli draws and an unknown parameter $p$ which specifies the probability of success for any Bernoulli trial.\n",
    "\n",
    "* The prior is specified by the beta distribution with parameters $\\alpha$ and $\\beta$\n",
    "* The likelihood is specified by the Binomial distribution with parameters $p$ and $n$\n",
    "\n",
    "The distribution functions associated with the prior and likelihood are given by\n",
    "\n",
    "* Likelihood, $P(k | p, n) = {n \\choose{k}} p^k (1 - p)^{n-k}$\n",
    "* Prior, $P(p) = \\frac{p^{\\alpha - 1} (1 - p)^{\\beta - 1}}{B(\\alpha, \\beta)}$\n",
    "\n",
    "Thus\n",
    "\n",
    "\\begin{align*}\n",
    "  P(p | k, \\alpha, \\beta, n) &\\propto {n \\choose{k}} p^k (1 - p)^{n-k} \\frac{p^{\\alpha - 1} (1 - p)^{\\beta - 1}}{B(\\alpha, \\beta)} \\\\\n",
    "  &\\propto p^{k + \\alpha - 1} (1 - p)^{n - k + \\beta - 1} \\frac{{n \\choose{k}}}{B(\\alpha, \\beta)} \\\\\n",
    "  &\\dots \\\\\n",
    "\\end{align*}\n",
    "\n",
    "If one were to finish writing out the algebra, you would fine that the posterior is given by a $\\text{Beta}(\\alpha + k, \\beta + (n - k))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141aa1c7-8082-4134-84e8-fde54a9dea90",
   "metadata": {},
   "source": [
    "**Example**:\n",
    "\n",
    "Imagine that we'd like to know how likely a student is to pass their PhD qualifying exams.\n",
    "\n",
    "* The PhD program only admits students that they think are likely to pass, so we begin with a\n",
    "  prior $P(p) \\sim \\text{Beta}(8, 2)$\n",
    "* The student takes 8 classes prior to taking the qualifying exam and passes each class with the same probability that they pass the qualifying exams\n",
    "* The student has successfully passed 7 of their 8 classes\n",
    "\n",
    "Since we are using a conjugate-prior, the posterior can be written as\n",
    "\n",
    "$$P(\\theta | k, \\alpha, \\beta, n) = \\text{Beta}(\\alpha + k, \\beta + 1)$$\n",
    "\n",
    "We can generate this posterior in Python using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1a535-987a-4879-822b-2e661e36b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "alpha, beta = 8, 2\n",
    "n = 8\n",
    "\n",
    "# Data\n",
    "k = 7\n",
    "\n",
    "bb_prior = st.beta(alpha, beta)\n",
    "bb_posterior = st.beta(alpha + k, beta + (n-k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f94cc5-c28b-43f1-8dce-b29f34bcd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "p = np.linspace(0, 1, 100)\n",
    "\n",
    "f_prior = bb_prior.pdf(p)\n",
    "f_posterior = bb_posterior.pdf(p)\n",
    "\n",
    "ax.plot(p, f_prior, \"k\")\n",
    "ax.plot(p, f_posterior, \"k--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e30c5-1ec0-4f5c-9e87-2b3c1372369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_posterior.rvs(1000).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c0347-d549-47c0-95a6-1716a415d5e1",
   "metadata": {},
   "source": [
    "### \"New school\" Bayesian stats\n",
    "\n",
    "Old school Bayesian statistics has one big limitation -- You have to choose distributions that fit into this relatively small set of conjugate-priors, however, there are lots of interesting problems that don't fit into this set...\n",
    "\n",
    "So how could we use Bayesian statistics to solve some of these problems?\n",
    "\n",
    "Would it be enough if we could sample from the posterior? If so, how could I sample from the posterior?\n",
    "\n",
    "Markov chain Monte Carlo (MCMC) methods allow us to do exactly this.\n",
    "\n",
    "Two papers initially proposed a version of this.\n",
    "\n",
    "1. _Equation of State Calculations by Fast Computing Machines_ by Nicholas Metropolis, Arianna W. Rosenbluth, Marshall Rosenbluth, Augusta H. Teller, Edward Teller\n",
    "2. _Monte Carlo Sampling Methods Using Markov Chains and Their Applications_ by W. K. Hastings\n",
    "\n",
    "The key idea of MCMC methods is to construct a Markov chain such that the stationary distribution of the Markov chain corresponds to the posterior distribution so if we sample from the Markov chain's stationary distribution then we're drawing samples from our posterior!\n",
    "\n",
    "[Online sampling tool](https://chi-feng.github.io/mcmc-demo/app.html)\n",
    "\n",
    "The question remains, how can we construct this \"magic\" Markov chain that has a stationary distribution that happens to be the same as our posterior...\n",
    "\n",
    "We're going to let \"probabilistic programming languages\", namely, [numpyro](https://num.pyro.ai/en/stable/) do this for us.\n",
    "\n",
    "> Note: [PyMC](https://www.pymc.io/welcome.html) is another great probabilistic programming language that we've used quite a bit -- I'm covering numpyro today largely because of a [blog post by Bob Carpenter](https://statmodeling.stat.columbia.edu/2025/10/03/its-a-jax-jax-jax-jax-world/) about numpyro's performance dominance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a753ec-a4ee-4487-9bc9-ed1077f1e29e",
   "metadata": {},
   "source": [
    "#### Simple example\n",
    "\n",
    "We're going to start exploring how to do this in a very simple model and will then do a more interesting model.\n",
    "\n",
    "Consider the following model of GDP growth:\n",
    "\n",
    "$$y_t = \\theta_y + \\sigma \\varepsilon_t$$\n",
    "\n",
    "\n",
    "where $\\varepsilon_t \\sim N(0, 1)$ and suppose we know that $\\sigma = 1.5$.\n",
    "\n",
    "Our goal is to find the posterior of the parameter $\\theta_y$.\n",
    "\n",
    "We choose a prior over $\\theta_y$ of $f(\\theta_y) = N(1, 4)$ and the likelihood that we derive from our model above is $f(y_t | \\theta) \\sim N(\\theta_y, 1.5^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90380f4-0607-46cb-9e68-cf965fc0cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDP growth\n",
    "y = np.array([\n",
    "    2.18, 1.49, 0.92, -0.17, 0.51, 1.52, 1.29, 0.94, 1.57, 1.69, 1.48, 1.74,\n",
    "    0.73, 1.18, 1.07, 1.91, 1.45, 1.84, 1.16, 1.69, 0.9, 0.78, 1.35, 1.16,\n",
    "    1.23, 2.09, 1.23, 1.58, 1.25, 1.87, 1.69, 1.19, 1.15, 1.16, 1.69, 1.9,\n",
    "    1.33, 1.14, 1.66, 2.25, 1.05, 2.45, 0.7, 1.16, 0.32, 1.19, -0.01, 0.6,\n",
    "    1.21, 0.97, 0.91, 0.72, 1.01, 1.16, 2.25, 1.75, 1.28, 1.58, 1.61, 1.78,\n",
    "    1.91, 1.17, 1.8, 1.44, 2.04, 1.07, 0.86, 1.22, 1.22, 1.22, 1.06, 1.01,\n",
    "    -0.21, 1.06, 0.2, -1.86, -1.13, -0.29, 0.47, 1.44, 0.64, 1.39, 1.03, 1.07,\n",
    "    0.3, 1.38, 0.62, 1.31, 1.41, 0.83, 0.65, 0.63, 1.29, 0.41, 1.27, 1.39,\n",
    "    0.13, 1.92, 1.66, 0.72, 0.86, 1.22, 0.68, 0.17\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73acafc8-d4ce-406a-bc37-6ef4cb2e3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdp_model(data=None):\n",
    "    # Prior for the mean GDP growth\n",
    "    theta_y = numpyro.sample(\"theta_y\", dist.Normal(1.0, 2.0))\n",
    "    \n",
    "    # Likelihood with fixed sigma=1.5\n",
    "    numpyro.sample(\"obs_ys\", dist.Normal(theta_y, 1.5), obs=data)\n",
    "\n",
    "# Random keys for JAX\n",
    "rng_key = jax.random.PRNGKey(20251203)\n",
    "rng_key, rng_key_infer, rng_key_prior, rng_key_post = jax.random.split(rng_key, 4)\n",
    "\n",
    "gdp_kernel = NUTS(gdp_model)\n",
    "gdp_mcmc = MCMC(gdp_kernel, num_warmup=1000, num_samples=2000)\n",
    "gdp_mcmc.run(rng_key_infer, data=y)\n",
    "gdp_posterior_samples = gdp_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f06e5-85c0-4369-a94a-c43d40f1e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c6953f-cfc4-4caa-b0a0-96eb9202d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_predictive = Predictive(gdp_model, num_samples=2000)\n",
    "prior_samples = prior_predictive(rng_key_prior, data=None)\n",
    "\n",
    "posterior_predictive = Predictive(gdp_model, posterior_samples=gdp_posterior_samples)\n",
    "post_pred_samples = posterior_predictive(rng_key_post, data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d505d-2d86-4561-8eff-11ee27653390",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "\n",
    "# 1. Top Plot: Posterior Predictive (Spans both columns)\n",
    "ax_top = fig.add_subplot(gs[0, :])\n",
    "az.plot_dist(\n",
    "    prior_samples['obs_ys'].flatten(),\n",
    "    ax=ax_top, color=\"C1\", label=\"Prior Predictive\"\n",
    ")\n",
    "az.plot_dist(\n",
    "    post_pred_samples['obs_ys'].flatten(),\n",
    "    ax=ax_top, color=\"C2\", label=\"Posterior Predictive\"\n",
    ")\n",
    "ax_top.hist(y, label=\"Observed Data\", density=True)\n",
    "ax_top.set_title(\"Posterior Predictive Distribution (GDP Growth)\")\n",
    "ax_top.set_xlabel(\"GDP Growth (%)\")\n",
    "ax_top.legend()\n",
    "\n",
    "# 2. Bottom Left: Prior vs Posterior (Parameters)\n",
    "ax_bl = fig.add_subplot(gs[1, 0])\n",
    "# Plot Prior density for theta_y\n",
    "az.plot_dist(prior_samples['theta_y'], ax=ax_bl, color=\"C0\", label=\"Prior\")\n",
    "# Plot Posterior density for theta_y\n",
    "az.plot_dist(gdp_posterior_samples['theta_y'], ax=ax_bl, color=\"C2\", label=\"Posterior\")\n",
    "ax_bl.set_title(\"Prior vs Posterior Distribution of $\\\\theta_y$\")\n",
    "ax_bl.set_xlabel(\"$\\\\theta_y$ Value\")\n",
    "ax_bl.legend()\n",
    "\n",
    "# 3. Bottom Right: Time-series of traces\n",
    "ax_br = fig.add_subplot(gs[1, 1])\n",
    "ax_br.plot(gdp_posterior_samples['theta_y'], alpha=0.7, color=\"C2\")\n",
    "ax_br.set_title(\"Trace of $\\\\theta_y$\")\n",
    "ax_br.set_xlabel(\"Iteration\")\n",
    "ax_br.set_ylabel(\"Parameter Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3e017-d787-44af-9b76-57ec61fdb0ad",
   "metadata": {},
   "source": [
    "#### CAPM example\n",
    "\n",
    "Now we do a slightly more interesting model. We will build a Bayesian version of the CAPM regression.\n",
    "\n",
    "Our model will be described by a few versions of the following equation:\n",
    "\n",
    "\\begin{align*}\n",
    "   r_{i, t} - r_{f, t} &= \\beta_i (r_{m, t} - r_{f, t}) + \\sigma_i \\varepsilon_{i, t}\\\\\n",
    "   \\sigma_i &\\sim \\text{HalfNormal}(4) \\\\\n",
    "   \\beta_i &\\sim N(0, 5)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3618dea-f9e4-4ced-bc55-c3a821c610b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for regression\n",
    "data = pd.read_parquet(\n",
    "    \"https://rice.box.com/shared/static/wbwwg1336g343bauhal74ryz365v0gge.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabea6d8-879c-4bcd-a678-b0bdf3e8ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be992a-74bd-4885-9c71-fcd4367950de",
   "metadata": {},
   "source": [
    "##### Estimating CAPM a single stock at a time\n",
    "\n",
    "We could estimate the CAPM for a single stock at a time -- Here we estimate the $\\beta$ for Zoom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4fd7a-417f-4215-a6f9-2b9e895318ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "zm_returns = data.query(\"ticker == 'ZM'\")\n",
    "ri_m_rf = zm_returns.eval(\"returns - riskfree\").to_numpy()\n",
    "rm_m_rf = zm_returns.eval(\"market - riskfree\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33400921-2caf-4ba5-af0a-50345f467ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capm_zm(ri_m_rf, rm_m_rf):\n",
    "    # CAPM parameters\n",
    "    beta_i = numpyro.sample(\n",
    "        \"beta_i\",\n",
    "        dist.Normal(0.0, 3.0)\n",
    "    )\n",
    "    sigma_i = numpyro.sample(\n",
    "        \"sigma_i\",\n",
    "        dist.HalfNormal(4.0)\n",
    "    )\n",
    "\n",
    "    # Likelihood\n",
    "    ll = numpyro.sample(\"ll\", dist.Normal(beta_i*rm_m_rf, sigma_i), obs=ri_m_rf)\n",
    "\n",
    "\n",
    "# Random keys for JAX\n",
    "rng_key = jax.random.PRNGKey(20251203)\n",
    "\n",
    "capm_zm_kernel = NUTS(capm_zm)\n",
    "capm_zm_mcmc = MCMC(capm_zm_kernel, num_warmup=1000, num_samples=2000)\n",
    "capm_zm_mcmc.run(rng_key, ri_m_rf=ri_m_rf, rm_m_rf=rm_m_rf)\n",
    "\n",
    "capm_zm_posterior_samples = capm_zm_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef858b07-aeed-4502-a651-e51ce502854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "capm_zm_posterior_samples[\"beta_i\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e085912-44a5-4685-9339-e14d1427077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "capm_zm_posterior_samples[\"beta_i\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229dbbd1-f340-4b39-99d9-df0ee693fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "capm_zm_posterior_samples[\"sigma_i\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c1f82-4842-4fb3-aa5e-42c682fb410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "capm_zm_posterior_samples[\"sigma_i\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af36b7f-a88f-4490-945d-73b30b539c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(az.from_numpyro(posterior=capm_zm_mcmc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7717854-556a-4d2d-bc78-f3d96822fb18",
   "metadata": {},
   "source": [
    "Zoom is estimated as having a slightly negative $\\beta$! What does this mean for a stock?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003961c-06f1-4464-a58e-8c05229edf9b",
   "metadata": {},
   "source": [
    "##### Estimating all \"amnesia\" models at once\n",
    "\n",
    "We could estimate $\\beta_i$ for all stocks in the SP500 by doing a \"vectorized\" like calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ff012-b33d-49ff-a55f-48edd523df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data\n",
    "tickers = data[\"ticker\"].unique()\n",
    "ntickers = tickers.shape[0]\n",
    "ticker_2_int = dict(zip(tickers, range(ntickers)))\n",
    "int_2_ticker = {v: k for k, v in ticker_2_int.items()}\n",
    "\n",
    "ri_m_rf = data.eval(\"returns - riskfree\").to_numpy()\n",
    "rm_m_rf = data.eval(\"market - riskfree\").to_numpy()\n",
    "ticker_idx = data[\"ticker\"].map(lambda x: ticker_2_int[x]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96ecd6-6256-4e22-aa0e-990c1b89c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_2_int[\"AAPL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc907f9-a1d1-436b-ab93-6e37994502b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_2_ticker[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e58bb-87df-43dd-a09d-e4a886061622",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a19bc-8aeb-4ab5-a190-145c144025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capm_amnesia(rm_m_rf, ri_m_rf, ticker_idx, ntickers):\n",
    "    with numpyro.plate(\"ticker_plate\", ntickers):    \n",
    "        beta_i = numpyro.sample(\n",
    "            \"beta_i\", \n",
    "            dist.Normal(loc=0.0, scale=3.0)\n",
    "        )\n",
    "    \n",
    "        sigma_i = numpyro.sample(\n",
    "            \"sigma_i\",\n",
    "            dist.HalfNormal(4.0)\n",
    "        )\n",
    "\n",
    "    # The mean (loc) for the Normal distribution.\n",
    "    indexed_beta = beta_i[ticker_idx]\n",
    "    mu = indexed_beta * rm_m_rf \n",
    "\n",
    "    # Select the correct sigma (scale) for each observation using the index array.\n",
    "    # indexed_sigma has shape (N,)\n",
    "    indexed_sigma = sigma_i[ticker_idx]\n",
    "\n",
    "    # 4. Likelihood\n",
    "    # This defines the likelihood for all N observations simultaneously.\n",
    "    numpyro.sample(\n",
    "        \"ll\", \n",
    "        dist.Normal(loc=mu, scale=indexed_sigma), \n",
    "        obs=ri_m_rf\n",
    "    )\n",
    "\n",
    "\n",
    "# Random keys for JAX\n",
    "rng_key = jax.random.PRNGKey(20251203)\n",
    "\n",
    "capm_amnesia_kernel = NUTS(capm_amnesia)\n",
    "capm_amnesia_mcmc = MCMC(capm_amnesia_kernel, num_warmup=1000, num_samples=2000)\n",
    "capm_amnesia_mcmc.run(\n",
    "    rng_key,\n",
    "    ri_m_rf=ri_m_rf, rm_m_rf=rm_m_rf,\n",
    "    ticker_idx=ticker_idx, ntickers=ntickers\n",
    ")\n",
    "\n",
    "capm_amnesia_posterior_samples = capm_amnesia_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5d8fc-cbae-49f0-a75a-1088eb4c6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very few stocks have a beta <0... Do we actually believe our ZM estimate?\n",
    "(capm_amnesia_posterior_samples[\"beta_i\"].mean(axis=0) < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e612d-5e46-4e7e-96f1-d06a4a7da1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(\n",
    "    capm_amnesia_posterior_samples[\"beta_i\"].mean(axis=0),\n",
    "    bins=[-0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0],\n",
    "    density=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5d42e-9254-4864-94fd-3f2ec8f0cffe",
   "metadata": {},
   "source": [
    "##### CAPM hierarchical model\n",
    "\n",
    "The purpose of the hierarchical model is to allow groups of observations to learn from one another -- We have, at most, 60 observations for each of our stocks because we are computing the 5 year beta (similar to what Yahoo Finance reports).\n",
    "\n",
    "To help with this, we are going to introduce two new notions:\n",
    "\n",
    "* A _hyperparameter_ is a parameter that's an input to a prior. For example, in our previous example we specified $\\beta_i \\sim N(0, 5)$ so 0 and 5 were hyperparameters.\n",
    "* A _hyperprior_ is a prior on a hyperparameter\n",
    "\n",
    "Hyperpriors will be a central feature of hierarchical models and they will be used to group observations. In our example, we originally wrote the following model\n",
    "\n",
    "\\begin{align*}\n",
    "   r_{i, t} - r_{f, t} &= \\beta_i (r_{m, t} - r_{f, t}) + \\sigma_i \\varepsilon_{i, t}\\\\\n",
    "   \\beta_i &\\sim N(0, 5) \\\\\n",
    "   \\sigma_i &\\sim \\text{HalfNormal}(4)\n",
    "\\end{align*}\n",
    "\n",
    "A hierarchical version of the model might be specified as\n",
    "\n",
    "\\begin{align*}\n",
    "   r_{i, t} - r_{f, t} &= \\beta_i (r_{m, t} - r_{f, t}) + \\sigma_i \\varepsilon_{i, t}\\\\\n",
    "   \\sigma_i &\\sim \\text{HalfNormal}(4) \\\\\n",
    "   \\beta_i &\\sim N(\\hat{\\mu}_j, \\hat{\\sigma}_j) \\\\\n",
    "   \\hat{\\mu}_j &\\sim N(0, 5) \\\\\n",
    "   \\hat{\\sigma}_j &\\sim \\text{HalfNormal}(4)\n",
    "\\end{align*}\n",
    "\n",
    "where $j$ could indicate the GICS sector (`gics`) that $i$ is identified by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a153d31-fd9c-4806-8dd0-fae1c8cb52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc7c95-fb09-46bb-ba6b-e45a9ddaa466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticker/subindustry\n",
    "_tick_sect = data.loc[:, [\"ticker\", \"gics\"]]\n",
    "\n",
    "# Note -- Duplicated tells us True anywhere that is a duplicate, except\n",
    "# for the first entry -- This gives us only the unique ticker/gics\n",
    "tick_sect = _tick_sect.loc[~_tick_sect.duplicated(keep=\"first\"), :]\n",
    "\n",
    "tickers = tick_sect[\"ticker\"].to_numpy()\n",
    "ntickers = tickers.shape[0]\n",
    "sects = tick_sect[\"gics\"].unique()\n",
    "nsect = sects.shape[0]\n",
    "\n",
    "# Mappings\n",
    "ticker_2_int = dict(zip(tickers, range(ntickers)))\n",
    "int_2_ticker = {v: k for k, v in ticker_2_int.items()}  # Only reverse when unique\n",
    "sect_2_int = dict(zip(sects, range(nsect)))\n",
    "int_2_sect = {v: k for k, v in sect_2_int.items()}  # Only reverse when unique\n",
    "ticker_2_sect = dict(\n",
    "    zip(\n",
    "        tick_sect[\"ticker\"].map(ticker_2_int).to_numpy(),\n",
    "        tick_sect[\"gics\"].map(sect_2_int).to_numpy()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Data\n",
    "ri_m_rf = data.eval(\"returns - riskfree\").to_numpy()\n",
    "rm_m_rf = data.eval(\"market - riskfree\").to_numpy()\n",
    "ticker_idx = data[\"ticker\"].map(lambda x: ticker_2_int[x]).to_numpy()\n",
    "sect_idx = np.array([ticker_2_sect[x] for x in range(ntickers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9a539-2d07-4e1c-967b-a10175925319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capm_hierarchical(\n",
    "    rm_m_rf,\n",
    "    ri_m_rf,\n",
    "    ticker_idx,\n",
    "    sect_idx,\n",
    "    ntickers,\n",
    "    nsect\n",
    "):\n",
    "    with numpyro.plate(\"group_prior_plate\", nsect):\n",
    "        # Hyperprior for the mean of Beta in each sector (mu_hat)\n",
    "        mu_hat = numpyro.sample(\n",
    "            \"mu_hat\", \n",
    "            dist.Normal(0.0, 3.0)\n",
    "        )\n",
    "    \n",
    "        # Hyperprior for the standard deviation of Beta in each sector (sigma_hat)\n",
    "        sigma_hat = numpyro.sample(\n",
    "            \"sigma_hat\", \n",
    "            dist.HalfNormal(\n",
    "                4.0\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # We use a plate for the ntickers dimension.\n",
    "    with numpyro.plate(\"ticker_plate\", ntickers):\n",
    "        # Beta Prior: pm.Normal(\"beta_i\", muhat[_sect_idx], sigmahat[_sect_idx], shape=ntickers)\n",
    "        beta_i = numpyro.sample(\n",
    "            \"beta_i\", \n",
    "            dist.Normal(\n",
    "                loc=mu_hat[sect_idx], \n",
    "                scale=sigma_hat[sect_idx]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Sigma Prior: pm.HalfCauchy(\"sigma_i\", 5, shape=ntickers)\n",
    "        sigma_i = numpyro.sample(\n",
    "            \"sigma_i\",\n",
    "            dist.HalfNormal(4)\n",
    "        )\n",
    "\n",
    "    # Calculate the mean (mu) and scale (sigma) for every observation using the ticker index (ticker_idx)\n",
    "    mu = beta_i[ticker_idx] * rm_m_rf\n",
    "    scale = sigma_i[ticker_idx]\n",
    "    \n",
    "    # Define the likelihood for all observations simultaneously.\n",
    "    numpyro.sample(\n",
    "        \"ll\", \n",
    "        dist.Normal(loc=mu, scale=scale), \n",
    "        obs=ri_m_rf\n",
    "    )\n",
    "\n",
    "# Random keys for JAX\n",
    "rng_key = jax.random.PRNGKey(20251203)\n",
    "\n",
    "kernel = NUTS(capm_hierarchical)\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=2000)\n",
    "mcmc.run(\n",
    "    rng_key_infer,\n",
    "    ri_m_rf=ri_m_rf, rm_m_rf=rm_m_rf,\n",
    "    ticker_idx=ticker_idx, sect_idx=sect_idx,\n",
    "    ntickers=ntickers, nsect=nsect\n",
    ")\n",
    "\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfab9f6-0c1a-4094-8ea5-ae2ab37dee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_2_int[\"ZM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400b778-960e-4faa-a7db-419322291b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples[\"beta_i\"][:, 503].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a2251-4d3a-4a40-9f2b-3b939ea3bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "amnesia_means = capm_amnesia_posterior_samples[\"beta_i\"].mean(axis=0)\n",
    "hierarchial_means = posterior_samples[\"beta_i\"].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af8fdb-c868-4aad-80a8-8e8ffba2fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "amnesia_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0660f8-a8b4-4823-a3f6-fdbda3b8c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "want = (sect_idx == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07cceb-2414-46e7-b929-84940484b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query(\"ticker == 'ZM'\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac283f5-a9ac-42c7-890c-d36e6567d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_2_sect[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db843b3-9162-42b3-b32e-5cc5850d5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(np.arange(505)[want], amnesia_means[want], c=\"b\")\n",
    "ax.scatter(np.arange(505)[want], hierarchial_means[want], c=\"r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
