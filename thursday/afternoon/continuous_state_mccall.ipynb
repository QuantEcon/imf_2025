{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40932afc-72b7-43a1-af59-5715048382f3",
   "metadata": {},
   "source": [
    "# RL methods for continuous states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23ca82-42e7-4446-82b6-e0f10a1e8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import optax\n",
    "\n",
    "from jax import nn\n",
    "from typing import NamedTuple\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b8008-032f-4520-a090-528f328c8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(20251204)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021bc60-55ff-41aa-9b9c-1f0473118183",
   "metadata": {},
   "source": [
    "## Example model\n",
    "\n",
    "We're going to start with a McCall model with job separation. Wages are going to be drawn from a continuous distribution $F(w)$ -- We will choose to draw log wages from a normal distribution.\n",
    "\n",
    "If an individual accepts a wage $w$ then they begin working at that wage in the next period and have a probability $\\alpha$ of losing their job which takes effect in the next period. While the worker is unemployed, they receive unemployment compensation $c$.\n",
    "\n",
    "The individual consumes all of their income each period and has a utility function $u$ that satisfies the \"usual conditions\"\n",
    "\n",
    "**The state**\n",
    "\n",
    "Each period's state is going to be the current wage offer.\n",
    "\n",
    "Note that if we were to solve this with dynamic programming, then we would simply discretize the wage offers and solve over a grid of wages. We are not going to directly discretize in our solution -- If we did, we could solve this with discrete TD methods similar to what we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c9895-89af-4398-bf82-a78760a50f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NamedTuple):\n",
    "    \"\"\"\n",
    "    Stores parameters for the McCall model.\n",
    "    \"\"\"\n",
    "    α: float = 0.1\n",
    "    β: float = 0.96\n",
    "    c: float = 1.\n",
    "    μ_w: float = 0.0\n",
    "    σ_w: float = 1.0\n",
    "\n",
    "\n",
    "def u(c):\n",
    "    \"\"\"log utility function.\"\"\"\n",
    "    c = jnp.maximum(c, 1e-10)\n",
    "\n",
    "    return jnp.log(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82403378-a1fe-431c-89e0-02f70e52277a",
   "metadata": {},
   "source": [
    "**Neural network helper code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda50239-ca50-47bf-b18a-74f653b0fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerParams(NamedTuple):\n",
    "    \"\"\"\n",
    "    Stores parameters for one layer of the neural network.\n",
    "    \"\"\"\n",
    "    W: jnp.ndarray     # weights\n",
    "    b: jnp.ndarray     # biases\n",
    "\n",
    "\n",
    "def initialize_layer(in_dim, out_dim, key, s=None):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a single layer.\n",
    "    Use LeCun initialization.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        s = jnp.sqrt(1.0 / in_dim)\n",
    "\n",
    "    W_key, b_key = jax.random.split(key, 2)\n",
    "    W = jax.random.normal(W_key, (in_dim, out_dim)) * s\n",
    "    b = jax.random.normal(b_key, (out_dim,))\n",
    "\n",
    "    return LayerParams(W, b)\n",
    "\n",
    "\n",
    "def initialize_network(key, layer_sizes, layer_initial_s={}):\n",
    "    \"\"\"\n",
    "    Build a network by initializing all of the parameters.\n",
    "    A network is a list of LayerParams instances.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        key, subkey = jax.random.split(key)\n",
    "\n",
    "        layer = initialize_layer(\n",
    "            layer_sizes[i],\n",
    "            layer_sizes[i + 1],\n",
    "            subkey,\n",
    "            s=layer_initial_s.get(i, jnp.sqrt(1.0 / layer_sizes[i]))\n",
    "        )\n",
    "        params.append(layer)\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def forward_2d(params, w):\n",
    "    \"\"\"\n",
    "    Evaluate neural network policy: maps a wage w to \n",
    "    probabilities over actions [reject, accept].\n",
    "    \n",
    "    Returns a 2-element array with probabilities.\n",
    "    \"\"\"\n",
    "    σ = jax.nn.relu\n",
    "    x = jnp.array((w,))  # Make state a 1D array\n",
    "    \n",
    "    # Forward pass through hidden layers\n",
    "    for W, b in params[:-1]:\n",
    "        x = σ(x @ W + b)\n",
    "    \n",
    "    # Final layer outputs logits\n",
    "    W, b = params[-1]\n",
    "    logits = x @ W + b\n",
    "    \n",
    "    # Convert to probabilities using softmax\n",
    "    probs = jax.nn.softmax(logits)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "def forward_1d(params, w):\n",
    "    \"\"\"\n",
    "    Evaluate neural network policy: maps a wage w to \n",
    "    probabilities over actions [reject, accept].\n",
    "    \n",
    "    Returns a 2-element array with probabilities.\n",
    "    \"\"\"\n",
    "    σ = jax.nn.relu\n",
    "    x = jnp.array((w,))  # Make state a 1D array\n",
    "    \n",
    "    # Forward pass through hidden layers\n",
    "    for W, b in params[:-1]:\n",
    "        x = σ(x @ W + b)\n",
    "    \n",
    "    # Final layer output\n",
    "    W, b = params[-1]\n",
    "    out = x @ W + b\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d04294-d06b-4696-a316-32591705c233",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "\n",
    "In the TD learning examples we saw previously, we effectively had a \"look-up table\" that said \"if we have this particular state, then our value is this particular value\"... This relied heavily on us having discrete states.\n",
    "\n",
    "What happens when we have continuous states?\n",
    "\n",
    "Well we need a way to evaluate (and update!) the value at each state. We've discussed a number of ways that we can approximate functions and update their values.\n",
    "\n",
    "### Deep Q-Learning Algorithm\n",
    "\n",
    "Deep Q-Learning uses a neural network to approximate the Q-function $Q(s, a)$, which represents the expected return from taking action $a$ in state $s$ and following the optimal policy thereafter.\n",
    "\n",
    "**Key components:**\n",
    "\n",
    "1. **Q-Network**: A neural network $Q(s, a; \\theta)$ that approximates the optimal action-value function\n",
    "2. **Target Network**: A separate network $Q(s, a; \\theta^-)$ with periodically updated weights to stabilize training\n",
    "3. **Experience Replay**: Store transitions $(s, a, r, s')$ in a buffer and sample randomly for training\n",
    "4. **Epsilon-Greedy Exploration**: With probability $\\epsilon$, take random action; otherwise take $\\arg\\max_a Q(s, a)$\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. Initialize Q-network with random weights $\\theta$\n",
    "2. Initialize target network with $\\theta^- = \\theta$\n",
    "3. Initialize experience replay buffer $\\mathcal{D}$\n",
    "4. For each episode:\n",
    "   - Initialize state $s$\n",
    "   - For each timestep:\n",
    "     - Select action using $\\epsilon$-greedy: $a = \\begin{cases} \\text{random} & \\text{w.p. } \\epsilon \\\\ \\arg\\max_a Q(s, a; \\theta) & \\text{otherwise} \\end{cases}$\n",
    "     - Take action $a$, observe reward $r$ and next state $s'$\n",
    "     - Store transition $(s, a, r, s')$ in $\\mathcal{D}$\n",
    "     - Sample random minibatch from $\\mathcal{D}$\n",
    "     - For each transition in minibatch:\n",
    "       - Compute target: $y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$\n",
    "       - Update $\\theta$ by minimizing loss: $L = (Q(s, a; \\theta) - y)^2$\n",
    "     - Every $N$ steps, update target network: $\\theta^- \\leftarrow \\theta$\n",
    "\n",
    "**Why these components matter:**\n",
    "\n",
    "- **Target network**: Prevents instability from chasing a moving target (the Q-values)\n",
    "- **Experience replay**: Breaks correlations between consecutive samples, improves data efficiency\n",
    "- **Epsilon-greedy**: Balances exploration (finding better policies) with exploitation (using current policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81762c-13c2-4224-bc9c-2f9fce9da580",
   "metadata": {},
   "source": [
    "## Policy methods vs value methods\n",
    "\n",
    "The TD methods that we learned were focused on estimating the optimal value of being in a particular state - There was a policy function but it was simply derived as a function of the value function.\n",
    "\n",
    "Policy based methods are going to put the policy function front-and-center. We're also going to use policy based methods as a stepping stone to transition from discrete states to continuous states but we will keep actions discrete for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576228f-9a76-43ae-b557-c1f1ae3cc127",
   "metadata": {},
   "source": [
    "## Policy method\n",
    "\n",
    "Policy methods are going to start by having/guessing a policy function.\n",
    "\n",
    "Your instinct is probably to think of a policy function as a mapping from states to an action, something like\n",
    "\n",
    "$$\\pi^*(s): \\mathcal{R}^N \\rightarrow \\mathcal{A}$$\n",
    "\n",
    "However, instead of mapping directly to the action space, we're going to map to a probabliity distribution over the action space.\n",
    "\n",
    "This means our policies are going to be stochastic (or at least potentially stochastic).\n",
    "\n",
    "There are a couple of reasons for this:\n",
    "\n",
    "* We may not need to directly hard-code the explore piece of the policy (something like $\\epsilon$-greedy) because it is baked into the policy.\n",
    "* Policy based methods that output a distribution are going to play more nicely as we move into high dimensional action spaces or continuous action spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f4dc2-3e13-4817-a718-dd0e868cd696",
   "metadata": {},
   "source": [
    "Below we define an initial policy function that's a simple Jax neural network with one input (the wage) and two outputs (the probabilities of rejecting or accepting the job):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eff0ed-9de3-47e4-9f93-1d7b1481459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGConfig:\n",
    "    \"\"\"\n",
    "    Configuration for training the policy network.\n",
    "    \"\"\"\n",
    "    seed = 20251204\n",
    "    epochs = 1500\n",
    "    episode_length = 150\n",
    "    num_episodes = 250\n",
    "    layer_sizes = (1, 16, 2)\n",
    "    init_lr = 0.05\n",
    "    min_lr = 0.0005\n",
    "    warmup_steps = 500\n",
    "    decay_steps = 250\n",
    "\n",
    "\n",
    "config_pg = PGConfig()\n",
    "key_pg = jax.random.PRNGKey(config_pg.seed)\n",
    "\n",
    "params_pg = initialize_network(key_pg, config_pg.layer_sizes, layer_initial_s={1: 0.0001})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c072865-74f9-440d-9109-4d724eacad6f",
   "metadata": {},
   "source": [
    "Here we plot the probabilities across the wages for the initial policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf17f06-adda-4e24-8993-2ce9b3d98b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of wage values\n",
    "w_grid = jnp.linspace(0.0, 10.0, 100)\n",
    "\n",
    "# Vectorize the forward function to evaluate over the grid\n",
    "policy_vmap = jax.vmap(lambda w: forward_2d(params_pg, w))\n",
    "probs_grid = policy_vmap(w_grid)\n",
    "\n",
    "# Extract probabilities for accept (1)\n",
    "prob_accept = probs_grid[:, 1]\n",
    "\n",
    "# Plot the probabilities\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(w_grid, prob_accept, label='Accept', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Wage Offer', fontsize=12)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title('Initial Policy Function: Probability of Accepting Job Offer', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At low wage (w={w_grid[0]:.2f}): P(accept)={prob_accept[0]:.3f}\")\n",
    "print(f\"At medium wage (w={w_grid[50]:.2f}): P(accept)={prob_accept[50]:.3f}\")\n",
    "print(f\"At high wage (w={w_grid[-1]:.2f}): P(accept)={prob_accept[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc195065-296c-44ce-bba9-fc4c419cc01e",
   "metadata": {},
   "source": [
    "## Policy gradient algorithm\n",
    "\n",
    "The policy gradient algorithm looks like what John described this morning except, rather than have a deterministic policy, we have a potentially stochastic policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3957a3e-f702-4211-afd3-a247de609644",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=('episode_length',))\n",
    "def simulate_episode(key, params, model, episode_length):\n",
    "    \"\"\"\n",
    "    Simulate one episode following the policy defined by params.\n",
    "    Uses jax.lax.scan for efficient compilation.\n",
    "    \n",
    "    Returns:\n",
    "        rewards: Array of rewards received (length episode_length)\n",
    "        log_probs: Array of log probabilities of actions (length episode_length)\n",
    "        decision_mask: Binary mask indicating when decisions were made\n",
    "    \"\"\"\n",
    "    β, μ_w, σ_w, c, α = model.β, model.μ_w, model.σ_w, model.c, model.α\n",
    "    \n",
    "    def step_fn(carry, t):\n",
    "        key, employed, current_wage = carry\n",
    "        key, wage_key, action_key, separation_key = jax.random.split(key, 4)\n",
    "        \n",
    "        # Generate wage offer (always, even if employed)\n",
    "        log_wage_offer = μ_w + jax.random.normal(wage_key)*σ_w\n",
    "        wage_offer = jnp.exp(log_wage_offer)\n",
    "\n",
    "        # Determine if we make a decision this step\n",
    "        making_decision = ~employed\n",
    "        \n",
    "        # Get policy probabilities (computed even if not used)\n",
    "        probs = forward_2d(params, wage_offer)\n",
    "        \n",
    "        # Sample action (0=reject, 1=accept) -- `jax.random.categorical` takes\n",
    "        # logits, not probabilities\n",
    "        action = jax.random.categorical(action_key, jnp.log(probs + 1e-10))        \n",
    "        log_prob = jnp.log(probs[action] + 1e-10)\n",
    "\n",
    "        def unemployed_branch():\n",
    "            # When unemployed, always receive unemployment compensation this period\n",
    "            reward = u(c)  # Always get c this period when unemployed\n",
    "            \n",
    "            # Employment status changes based on decision\n",
    "            new_employed = action == 1\n",
    "            new_wage = wage_offer\n",
    "            \n",
    "            return reward, new_employed, new_wage\n",
    "\n",
    "        def employed_branch():\n",
    "            # Always receive current wage this period when employed\n",
    "            reward = u(current_wage)\n",
    "            \n",
    "            # Check if lose job (affects next period)\n",
    "            lose_job = jax.random.uniform(separation_key) < α\n",
    "            new_employed = ~lose_job\n",
    "            new_wage = current_wage\n",
    "            \n",
    "            return reward, new_employed, new_wage\n",
    "        \n",
    "        reward, new_employed, new_wage = jax.lax.cond(\n",
    "            employed,\n",
    "            employed_branch,\n",
    "            unemployed_branch\n",
    "        )\n",
    "\n",
    "        # Only use log_prob if we made a decision\n",
    "        log_prob_masked = jnp.where(making_decision, log_prob, 0.0)\n",
    "\n",
    "        new_carry = (key, new_employed, new_wage)\n",
    "        output = (reward, log_prob_masked, making_decision)\n",
    "\n",
    "        return new_carry, output\n",
    "    \n",
    "    # Initial state: unemployed, no wage\n",
    "    init_carry = (key, False, 0.0)\n",
    "    \n",
    "    # Run the scan\n",
    "    final_carry, outputs = jax.lax.scan(step_fn, init_carry, jnp.arange(episode_length))\n",
    "    \n",
    "    rewards, log_probs, decision_mask = outputs\n",
    "    \n",
    "    return rewards, log_probs, decision_mask\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_returns(rewards, β):\n",
    "    \"\"\"Compute discounted returns for each timestep using scan.\"\"\"\n",
    "    def scan_fn(G, reward):\n",
    "        new_G = reward + β * G\n",
    "        return new_G, new_G\n",
    "    \n",
    "    # Scan backwards through rewards\n",
    "    _, returns = jax.lax.scan(scan_fn, 0.0, rewards, reverse=True)\n",
    "    return returns\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=('episode_length', 'num_episodes'))\n",
    "def policy_gradient_loss(params, model, key, episode_length, num_episodes):\n",
    "    \"\"\"\n",
    "    Compute the REINFORCE policy gradient loss.\n",
    "    Uses vmap to vectorize over episodes for speed.\n",
    "    \"\"\"\n",
    "    # Generate keys for all episodes\n",
    "    keys = jax.random.split(key, num_episodes)\n",
    "    \n",
    "    # Vectorize simulate_episode over episodes\n",
    "    simulate_batch = jax.vmap(\n",
    "        lambda k: simulate_episode(k, params, model, episode_length)\n",
    "    )\n",
    "    \n",
    "    # Simulate all episodes in parallel\n",
    "    rewards_batch, log_probs_batch, decision_mask_batch = simulate_batch(keys)\n",
    "    \n",
    "    # Compute returns for all episodes\n",
    "    returns_batch = jax.vmap(lambda r: compute_returns(r, model.β))(rewards_batch)    \n",
    "    \n",
    "    # Compute loss: only include log_probs where decisions were made\n",
    "    # Multiply by decision_mask to zero out non-decision timesteps\n",
    "    losses = -jnp.sum(returns_batch * log_probs_batch * decision_mask_batch, axis=1)\n",
    "\n",
    "    return jnp.mean(losses)\n",
    "\n",
    "def create_lr_schedule(config):\n",
    "    \"\"\"Create learning rate schedule.\"\"\"\n",
    "    warmup_fn = optax.linear_schedule(\n",
    "        init_value=0.0,\n",
    "        end_value=config.init_lr,\n",
    "        transition_steps=config.warmup_steps\n",
    "    )\n",
    "    \n",
    "    decay_fn = optax.exponential_decay(\n",
    "        init_value=config.init_lr,\n",
    "        transition_steps=config.decay_steps,\n",
    "        decay_rate=0.99,\n",
    "        end_value=config.min_lr\n",
    "    )\n",
    "    \n",
    "    return optax.join_schedules(\n",
    "        schedules=[warmup_fn, decay_fn],\n",
    "        boundaries=[config.warmup_steps]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a3022-24e6-4eae-b25d-f2a9da5050d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "config_pg = PGConfig()\n",
    "model = Model()\n",
    "\n",
    "lr_schedule = create_lr_schedule(config_pg)\n",
    "optimizer = optax.adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Re-initialize parameters for training\n",
    "key_pg = jax.random.PRNGKey(config_pg.seed)\n",
    "params_pg = initialize_network(key_pg, config_pg.layer_sizes, layer_initial_s={1: 0.0001})\n",
    "opt_state = optimizer.init(params_pg)\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "best_loss = jnp.inf\n",
    "best_params = params_pg\n",
    "\n",
    "print(\"Training policy\")\n",
    "for epoch in range(config_pg.epochs):\n",
    "    key_pg, subkey = jax.random.split(key_pg)\n",
    "    \n",
    "    # Compute loss and gradients (JIT compiled!)\n",
    "    loss, grads = jax.value_and_grad(policy_gradient_loss)(\n",
    "        params_pg, model, subkey, config_pg.episode_length, config_pg.num_episodes\n",
    "    )\n",
    "\n",
    "    loss_history.append(float(loss))\n",
    "\n",
    "    # Track best parameters\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_params = params_pg\n",
    "\n",
    "    # Update parameters\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params_pg = optax.apply_updates(params_pg, updates)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{config_pg.epochs}: Loss = {loss:.4f}\")\n",
    "\n",
    "# Use best parameters\n",
    "params = params_pg\n",
    "print(f\"\\nTraining complete! Best loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13139178-9d9d-4233-860a-bee7219dcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning progress\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(loss_history, linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('REINFORCE Training Progress', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot learned policy\n",
    "w_grid_trained = jnp.linspace(0.0, 10.0, 100)\n",
    "policy_vmap_trained = jax.vmap(lambda w: forward_2d(params, w))\n",
    "probs_grid_trained = policy_vmap_trained(w_grid_trained)\n",
    "prob_accept_trained = probs_grid_trained[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(w_grid_trained, prob_accept_trained, linewidth=2, label='Trained Policy')\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "ax.set_xlabel('Wage Offer', fontsize=12)\n",
    "ax.set_ylabel('Probability of Accepting', fontsize=12)\n",
    "ax.set_title('Learned Policy Function (REINFORCE)', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPolicy characteristics:\")\n",
    "print(f\"At low wage (w={w_grid_trained[0]:.2f}): P(accept)={prob_accept_trained[0]:.3f}\")\n",
    "print(f\"At medium wage (w={w_grid_trained[50]:.2f}): P(accept)={prob_accept_trained[50]:.3f}\")\n",
    "print(f\"At high wage (w={w_grid_trained[-1]:.2f}): P(accept)={prob_accept_trained[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7955e-c964-4aff-8377-ef15358dfeab",
   "metadata": {},
   "source": [
    "## Actor-critic algorithm\n",
    "\n",
    "The policy methods described above depend quite heavily on sampling an entire sequence and then making a single update to the parameters which means that they can end up with a relatively high variance (which isn't particularly desirable). This also means that the policy gradient methods upweight (or downweight) _all_ of the actions taken on the generated history so your gradients aren't as useful as you might like since it's hard to tease out \"what were the good actions\" and \"what were the bad actions\"...\n",
    "\n",
    "To try to resolve these issues (and particularly the second one), methods like \"actor-critic\" have been developed.\n",
    "\n",
    "![Actor-Critic](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/ac.jpg)\n",
    "\n",
    "^Image credit to HuggingFace RL course\n",
    "\n",
    "In an actor-critic model, you train two networks -- One network for the policy function and one network for the value function. The network for the policy function (actor) is trained similarly to how we discussed in policy gradient methods but with one added feature, \"the critic\"\n",
    "\n",
    "The critic is simply a network that can evaluate whether the action taken by the agent was a good one or a bad one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cbfdee-ddd6-4eb0-b3c1-d7060eeea58b",
   "metadata": {},
   "source": [
    "**Advantage Actor Critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41341084-547a-4c8d-a7a2-597da420897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACConfig:\n",
    "    \"\"\"\n",
    "    Configuration for training the actor-critic network.\n",
    "    \"\"\"\n",
    "    seed = 20251204\n",
    "    epochs = 500\n",
    "    episode_length = 50\n",
    "    num_episodes = 250\n",
    "    actor_layer_sizes = (1, 32, 8, 32, 2)\n",
    "    critic_layer_sizes = (1, 32, 8, 32, 1)\n",
    "    init_lr = 0.05\n",
    "    min_lr = 0.0005\n",
    "    warmup_steps = 500\n",
    "    decay_steps = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df1583-534d-47b4-a50f-0e0df0701339",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=('episode_length',))\n",
    "def simulate_episode_ac(key, actor_params, critic_params, model, episode_length):\n",
    "    \"\"\"\n",
    "    Simulate one episode for actor-critic.\n",
    "    Computes TD errors on the fly with proper next-state value estimation.\n",
    "    \n",
    "    Returns:\n",
    "        rewards: Array of rewards\n",
    "        log_probs: Array of log probabilities\n",
    "        values: Array of state values\n",
    "        next_values: Array of next state values (for TD error)\n",
    "        decision_mask: Binary mask for decisions\n",
    "    \"\"\"\n",
    "    β, μ_w, σ_w, c, α = model.β, model.μ_w, model.σ_w, model.c, model.α\n",
    "    \n",
    "    def step_fn(carry, t):\n",
    "        key, employed, current_wage = carry\n",
    "        key, k1, k2, k3, k4 = jax.random.split(key, 5)\n",
    "        \n",
    "        # Generate current wage offer\n",
    "        log_wage_offer = μ_w + jax.random.normal(k1) * σ_w\n",
    "        wage_offer = jnp.exp(log_wage_offer)\n",
    "        \n",
    "        # Pre-generate NEXT wage offer\n",
    "        next_log_wage = μ_w + jax.random.normal(k4) * σ_w\n",
    "        next_wage_offer = jnp.exp(next_log_wage)\n",
    "        \n",
    "        # Determine if making decision\n",
    "        making_decision = ~employed\n",
    "        \n",
    "        # Get policy probabilities and value for current state\n",
    "        probs = forward_2d(actor_params, wage_offer)\n",
    "        value = forward_1d(critic_params, wage_offer)[0]\n",
    "        \n",
    "        # Sample action\n",
    "        action = jax.random.categorical(k2, jnp.log(probs + 1e-10))\n",
    "        log_prob = jnp.log(probs[action] + 1e-10)\n",
    "        \n",
    "        # Compute reward and next state\n",
    "        def unemployed_branch():\n",
    "            # When unemployed, always receive unemployment compensation this period\n",
    "            reward = u(c)\n",
    "            \n",
    "            # Employment status changes based on decision\n",
    "            new_employed = action == 1\n",
    "            new_wage = wage_offer\n",
    "            \n",
    "            # Next value: if accept, value of that wage; if reject, value of next draw\n",
    "            next_val_wage = jnp.where(action == 1, wage_offer, next_wage_offer)\n",
    "            return reward, new_employed, new_wage, next_val_wage\n",
    "        \n",
    "        def employed_branch():\n",
    "            # Always receive current wage this period when employed\n",
    "            reward = u(current_wage)\n",
    "            \n",
    "            # Check if lose job (affects next period)\n",
    "            lose_job = jax.random.uniform(k3) < α\n",
    "            new_employed = ~lose_job\n",
    "            new_wage = current_wage\n",
    "            \n",
    "            # Next value: if keep job, value of current wage; if lose, value of next draw\n",
    "            next_val_wage = jnp.where(lose_job, next_wage_offer, current_wage)\n",
    "            return reward, new_employed, new_wage, next_val_wage\n",
    "        \n",
    "        reward, new_employed, new_wage, next_val_wage = jax.lax.cond(\n",
    "            employed,\n",
    "            employed_branch,\n",
    "            unemployed_branch\n",
    "        )\n",
    "        \n",
    "        # Compute next state value\n",
    "        next_value = forward_1d(critic_params, next_val_wage)[0]\n",
    "        \n",
    "        # Mask outputs (only for decisions made)\n",
    "        log_prob_masked = jnp.where(making_decision, log_prob, 0.0)\n",
    "        value_masked = jnp.where(making_decision, value, 0.0)\n",
    "        next_value_masked = jnp.where(making_decision, next_value, 0.0)\n",
    "        \n",
    "        new_carry = (key, new_employed, new_wage)\n",
    "        output = (reward, log_prob_masked, value_masked, next_value_masked, making_decision)\n",
    "        \n",
    "        return new_carry, output\n",
    "    \n",
    "    init_carry = (key, False, 0.0)\n",
    "    final_carry, outputs = jax.lax.scan(step_fn, init_carry, jnp.arange(episode_length))\n",
    "    \n",
    "    rewards, log_probs, values, next_values, decision_mask = outputs\n",
    "    \n",
    "    return rewards, log_probs, values, next_values, decision_mask\n",
    "\n",
    "@partial(jax.jit, static_argnames=('episode_length', 'num_episodes'))\n",
    "def actor_critic_loss(actor_params, critic_params, model, key, episode_length, num_episodes):\n",
    "    \"\"\"\n",
    "    Compute actor-critic loss.\n",
    "    Actor loss: -log π(a|s) * advantage\n",
    "    Critic loss: (V(s) - target)^2\n",
    "    \"\"\"\n",
    "    keys = jax.random.split(key, num_episodes)\n",
    "    \n",
    "    # Vectorize over episodes\n",
    "    simulate_batch = jax.vmap(\n",
    "        lambda k: simulate_episode_ac(k, actor_params, critic_params, model, episode_length)\n",
    "    )\n",
    "    \n",
    "    # Simulate all episodes\n",
    "    rewards_batch, log_probs_batch, values_batch, next_values_batch, decision_mask_batch = simulate_batch(keys)\n",
    "    \n",
    "    # Compute TD errors (advantages)\n",
    "    # advantage = reward + γ * V(s') - V(s)\n",
    "    td_targets = rewards_batch + model.β * next_values_batch\n",
    "    advantages = td_targets - values_batch\n",
    "\n",
    "    # Actor loss: policy gradient with advantage\n",
    "    actor_losses = -jnp.sum(log_probs_batch * advantages * decision_mask_batch, axis=1)\n",
    "    \n",
    "    # Critic loss: MSE between value and TD target\n",
    "    critic_losses = jnp.sum(decision_mask_batch * (values_batch - td_targets)**2, axis=1)\n",
    "    \n",
    "    # Average over episodes\n",
    "    actor_loss = jnp.mean(actor_losses)\n",
    "    critic_loss = jnp.mean(critic_losses)\n",
    "    \n",
    "    return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d043e-04c5-403c-b349-96549584eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "config_ac = ACConfig()\n",
    "model_ac = Model()\n",
    "\n",
    "lr_schedule_ac = create_lr_schedule(config_ac)\n",
    "actor_optimizer = optax.adam(learning_rate=lr_schedule_ac)\n",
    "critic_optimizer = optax.adam(learning_rate=lr_schedule_ac)\n",
    "\n",
    "# Initialize actor and critic networks\n",
    "key_ac = jax.random.PRNGKey(config_ac.seed)\n",
    "key_ac, actor_key, critic_key = jax.random.split(key_ac, 3)\n",
    "\n",
    "actor_params = initialize_network(actor_key, config_ac.actor_layer_sizes, layer_initial_s={3: 0.0001})\n",
    "critic_params = initialize_network(critic_key, config_ac.critic_layer_sizes)\n",
    "\n",
    "actor_opt_state = actor_optimizer.init(actor_params)\n",
    "critic_opt_state = critic_optimizer.init(critic_params)\n",
    "\n",
    "# Training loop\n",
    "actor_loss_history = []\n",
    "critic_loss_history = []\n",
    "best_actor_loss = jnp.inf\n",
    "best_actor_params = actor_params\n",
    "best_critic_params = critic_params\n",
    "\n",
    "print(\"Training Actor-Critic...\")\n",
    "\n",
    "for epoch in range(config_ac.epochs):\n",
    "    key_ac, subkey = jax.random.split(key_ac)\n",
    "    \n",
    "    # Compute gradients for both actor and critic\n",
    "    def actor_loss_fn(a_params):\n",
    "        actor_l, critic_l = actor_critic_loss(\n",
    "            a_params, critic_params, model_ac, subkey, \n",
    "            config_ac.episode_length, config_ac.num_episodes\n",
    "        )\n",
    "        return actor_l\n",
    "    \n",
    "    def critic_loss_fn(c_params):\n",
    "        actor_l, critic_l = actor_critic_loss(\n",
    "            actor_params, c_params, model_ac, subkey, \n",
    "            config_ac.episode_length, config_ac.num_episodes\n",
    "        )\n",
    "        return critic_l\n",
    "    \n",
    "    # Compute losses and gradients\n",
    "    actor_loss, actor_grads = jax.value_and_grad(actor_loss_fn)(actor_params)\n",
    "    critic_loss, critic_grads = jax.value_and_grad(critic_loss_fn)(critic_params)\n",
    "    \n",
    "    actor_loss_history.append(float(actor_loss))\n",
    "    critic_loss_history.append(float(critic_loss))\n",
    "    \n",
    "    # Update parameters\n",
    "    actor_updates, actor_opt_state = actor_optimizer.update(actor_grads, actor_opt_state)\n",
    "    actor_params = optax.apply_updates(actor_params, actor_updates)\n",
    "    \n",
    "    critic_updates, critic_opt_state = critic_optimizer.update(critic_grads, critic_opt_state)\n",
    "    critic_params = optax.apply_updates(critic_params, critic_updates)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{config_ac.epochs}: \"\n",
    "              f\"Actor Loss = {actor_loss:.4f}, Critic Loss = {critic_loss:.4f}\")\n",
    "\n",
    "# Plot learning progress\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(actor_loss_history, linewidth=2, label='Actor Loss', alpha=0.8)\n",
    "ax.plot(critic_loss_history, linewidth=2, label='Critic Loss', alpha=0.8)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Actor-Critic Training Progress', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot learned policy and value function\n",
    "w_grid_ac = jnp.linspace(0.1, 10.0, 100)\n",
    "policy_vmap_ac = jax.vmap(lambda w: forward_2d(actor_params, w))\n",
    "value_vmap_ac = jax.vmap(lambda w: forward_1d(critic_params, w)[0])\n",
    "\n",
    "probs_grid_ac = policy_vmap_ac(w_grid_ac)\n",
    "prob_accept_ac = probs_grid_ac[:, 1]\n",
    "values_grid_ac = value_vmap_ac(w_grid_ac)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot policy\n",
    "ax1.plot(w_grid_ac, prob_accept_ac, linewidth=2, label='Actor-Critic Policy')\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "ax1.set_xlabel('Wage Offer', fontsize=12)\n",
    "ax1.set_ylabel('Probability of Accepting', fontsize=12)\n",
    "ax1.set_title('Learned Policy Function (Actor-Critic)', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot value function\n",
    "ax2.plot(w_grid_ac, values_grid_ac, linewidth=2, color='green', label='Value Function')\n",
    "ax2.set_xlabel('Wage Offer', fontsize=12)\n",
    "ax2.set_ylabel('State Value', fontsize=12)\n",
    "ax2.set_title('Learned Value Function (Critic)', fontsize=14)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPolicy characteristics:\")\n",
    "print(f\"At low wage (w={w_grid_ac[0]:.2f}): P(accept)={prob_accept_ac[0]:.3f}, V={values_grid_ac[0]:.3f}\")\n",
    "print(f\"At medium wage (w={w_grid_ac[50]:.2f}): P(accept)={prob_accept_ac[50]:.3f}, V={values_grid_ac[50]:.3f}\")\n",
    "print(f\"At high wage (w={w_grid_ac[-1]:.2f}): P(accept)={prob_accept_ac[-1]:.3f}, V={values_grid_ac[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcbdf04-d442-48d4-a684-6b952513b1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
