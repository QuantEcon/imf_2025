{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Getting started with reinforcement learning: TD learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (MDP)\n",
    "\n",
    "A MDP consists of\n",
    "\n",
    "* An agent, or decision maker, who observes the \"state\" ($S_t$) and each period chooses an action ($A_t$)\n",
    "* There is an environment that takes the current state and the action taken by the decision maker and produces a new state ($S_{t+1}$). The state follows the Markov property, i.e. $p(S_{t+1} | S^t, A_t) = p(S_{t+1} | S_t, A_t)$.\n",
    "* After selecting an action, the decision maker earns and a reward in the next period ($R_{t+1}$).\n",
    "* Agent seeks to maximize their \"lifetime\" (whether finite or infinite) discounted rewards.\n",
    "\n",
    "Macro models often fit into the category of a MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy and value functions\n",
    "\n",
    "Two objects of interest that we can recall \n",
    "\n",
    "**Policy function**\n",
    "\n",
    "A policy function, $\\pi(s)$ is a mapping from states to a probability distribution over actions.\n",
    "\n",
    "If an agent is following $\\pi$ then $\\pi(a | s)$ is the probability that the agent selection action $a$ when it observes state $s$.\n",
    "\n",
    "**Value function**\n",
    "\n",
    "A value function, $v_\\pi(s)$, is the value of being in state $s$ and following the specified policy function, $\\pi(s)$, into the future.\n",
    "\n",
    "$$v_\\pi(s) = E \\left[ \\sum_{\\tau=0}^\\infty \\gamma^{\\tau} R^\\pi_{t+\\tau} \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-value function\n",
    "\n",
    "We introduce a new version of the value function called the _action-value function_. The action-value function representes the value of being in state $s$ and choosing to take action $a$ today and then following $\\pi(s)$ into the future\n",
    "\n",
    "$$q_\\pi(s, a) = E \\left[ R_{t+1}(a, s) + \\sum_{\\tau=1}^\\infty \\gamma^{\\tau} R^\\pi_{t+\\tau} \\right]$$\n",
    "\n",
    "We can think about the value function in terms of the action-value function by writing:\n",
    "\n",
    "$$v_\\pi(s) = \\arg \\max_a q_\\pi(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the solution?\n",
    "\n",
    "The solution to a MDP is an optimal policy function ($\\pi^*$) and an optimal value function ($v^*$).\n",
    "\n",
    "The solutions satisfy the Bellman equation given by\n",
    "\n",
    "$$v^*(s) = E \\left[ R_{t+1}(s, \\pi^*(s)) + \\gamma v^*(s') \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the solution with dynamic programming\n",
    "\n",
    "We saw with John some ways that we can find the solutions $v^*$ and $\\pi^*$ -- The key thing I want to remind us of is that the algorithm went something like:\n",
    "\n",
    "1. Take an initial guess at $v^*$\n",
    "2. Iterate through all of the states and find the optimal action -- Use this to update your guess of $v^*$.\n",
    "3. Check whether $v^*$ is roughly the same and, if it is, then you've found a solution\n",
    "\n",
    "There are variations on this algorithm but they roughly \n",
    "\n",
    "**How reinforcement learning is going to differ**\n",
    "\n",
    "Reinforcement learning is going to slightly differ on two dimensions:\n",
    "\n",
    "1. We assume that the decision maker understands all of the probability distributions that govern the model in dynamic programming but, in reinforcement learning, the only information that the decision maker receives is the rewards that they get.\n",
    "2. For each \"update pass\" that we make, we are not going to iterate through \"all of the possible states\" -- The agent simply is going to \"interact with the environment.\n",
    "\n",
    "_Note_: There are some clever papers that work to reduce the number of states that you iterate through in dynamic programming as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Temporal difference (TD) learning\n",
    "\n",
    "In the spirit of \"walk before you run\", we are going to start by learning about a discrete setting where:\n",
    "\n",
    "* The set of states $\\mathcal{S}$ is discrete.\n",
    "* The set of actions $\\mathcal{A}(S_t)$ is discrete.\n",
    "\n",
    "This means that our policy function and our value function are effectively just \"look-up\" tables.\n",
    "\n",
    "Let's rewrite our action-state value function in the optimal format and do some rearranging:\n",
    "\n",
    "\\begin{align*}\n",
    "  Q(s_t, a_t) = E \\left[ R_{t+1} + \\gamma \\max_{a_{t+1}} Q(s_{t+1}, a_{t+1}) | s_t, a_t \\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now suppose that we had some tuple of $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$ that came from \"somewhere\".\n",
    "\n",
    "We could define $\\delta_t$ as\n",
    "\n",
    "\\begin{align*}\n",
    "  \\delta_t = R_{t+1} + (\\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))\n",
    "\\end{align*}\n",
    "\n",
    "Notice that this is a _sample value_ not an evaluation of the expectation -- It gives us some sense of whether $Q(s_t, a_t)$ was too high or too low but we shouldn't expect $\\delta_t$ to be 0... We just expect that if we did this lots of times then it would be 0 in expectation.\n",
    "\n",
    "TD learning methods are going to use $\\delta_t$ as a way to update our beliefs about the action-state value function. More specifically, once we have $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$ and $\\delta_t$ we are going to update our action-state value function by doing a TD(0) update:\n",
    "\n",
    "\\begin{align*}\n",
    "  Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\delta_t\n",
    "\\end{align*}\n",
    "\n",
    "where $\\alpha$ is something like a \"learning rate\" like you would have in an algorithm like gradient descent. It ensures that steps aren't too big and that we don't get oscillatory behavior.\n",
    "\n",
    "> There are extensions to the temporal difference that allow for multiple time periods. The 0 in $TD(0)$ indicates that this is *one-step* TD learning. See Chapters 7 and 12 of Sutton/Barto for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliff-walking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnvironment:\n",
    "    def __init__(self, height=4, width=12):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        self.start_loc = (0, 0)\n",
    "        self.end_loc = (width-1, 0)\n",
    "\n",
    "        self.cliff = [(i, 0) for i in range(1, width-1)]\n",
    "\n",
    "        self.loc = self.start_loc\n",
    "        self.total_cost = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.loc = self.start_loc\n",
    "        self.total_cost = 0\n",
    "\n",
    "    def is_inbounds(self, loc):\n",
    "        x, y = loc\n",
    "\n",
    "        inbounds = True\n",
    "        if x < 0:\n",
    "            inbounds = False\n",
    "        if x >= self.width:\n",
    "            inbounds = False\n",
    "        if y < 0:\n",
    "            inbounds = False\n",
    "        if y >= self.height:\n",
    "            inbounds = False\n",
    "\n",
    "        return inbounds\n",
    "\n",
    "    def loc_update(self, loc, action):\n",
    "        x, y = loc\n",
    "\n",
    "        # Left\n",
    "        if action == 0:\n",
    "            x = x - 1\n",
    "        # Right\n",
    "        elif action == 1:\n",
    "            x = x + 1\n",
    "        # Up\n",
    "        elif action == 2:\n",
    "            y = y + 1\n",
    "        # Down\n",
    "        else:\n",
    "            y = y - 1\n",
    "\n",
    "        return (x, y)\n",
    "\n",
    "    def enumerate_options(self, loc):\n",
    "        actions = []\n",
    "\n",
    "        for action in [0, 1, 2, 3]:\n",
    "            if self.is_inbounds(self.loc_update(loc, action)):\n",
    "                actions.append(action)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.loc\n",
    "\n",
    "        # Update location\n",
    "        xu, yu = self.loc_update((x, y), action)\n",
    "\n",
    "        # Force inbounds -- If action would move you out of bounds\n",
    "        # then leave the agent where they were\n",
    "        inbounds = self.is_inbounds((xu, yu))\n",
    "        if not inbounds:\n",
    "            xu, yu = x, y\n",
    "\n",
    "        # Figure out the penalty\n",
    "        penalty = 1\n",
    "        if (xu, yu) in self.cliff:\n",
    "            penalty = 100\n",
    "\n",
    "            # If you fall off the cliff, go back to the\n",
    "            # starting position\n",
    "            xu, yu = self.start_loc\n",
    "        self.total_cost += penalty\n",
    "\n",
    "        # Check if we're finished\n",
    "        self.loc = (xu, yu)\n",
    "        terminated = False\n",
    "        if (xu, yu) == self.end_loc:\n",
    "            terminated = True\n",
    "\n",
    "        return (xu, yu), penalty, terminated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing $Q(s_t, a_t)$\n",
    "\n",
    "Again, as stated above, everything in our model is going to initially be discrete. Discrete states. Discrete actions.\n",
    "\n",
    "We will represent Q with a matrix -- It can be something along the lines of `(n_states, n_actions)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sarsa Algorithm\n",
    "\n",
    "- The Sarsa algorithm is one version of a TD algorithm\n",
    "- The algorithm is summarized by Barto and Sutton as follows (section 6.4)\n",
    "\n",
    "![sarsa_barto_sutton.png](https://compsosci-resources.s3.amazonaws.com/images/sarsa_barto_sutton.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize a SARSA agent.\n",
    "        \n",
    "        Args:\n",
    "            env: The environment the agent will interact with\n",
    "            alpha: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon: Exploration rate\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        # State is (x, y) and actions are 0, 1, 2, 3 (left, right, up, down)\n",
    "        self.Q = np.zeros((env.width, env.height, 4))\n",
    "        \n",
    "        # For tracking learning progress\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "    \n",
    "    def choose_action(self, state, explore=True):\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (x, y)\n",
    "            explore: Whether to use exploration or not\n",
    "            \n",
    "        Returns:\n",
    "            The chosen action\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        \n",
    "        # Get valid actions from the environment\n",
    "        valid_actions = self.env.enumerate_options(state)\n",
    "        \n",
    "        if explore and random.random() < self.epsilon:\n",
    "            # Exploration: choose a random valid action\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            # Exploitation: choose the best action (with random tie-breaking)\n",
    "            q_values = [self.Q[x, y, a] if a in valid_actions else -np.inf for a in range(4)]\n",
    "            max_q = max([q_values[a] for a in valid_actions])\n",
    "            best_actions = [a for a in valid_actions if q_values[a] == max_q]\n",
    "            action = random.choice(best_actions)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def train(self, num_episodes=500, verbose=False):\n",
    "        \"\"\"\n",
    "        Train the agent using SARSA algorithm.\n",
    "        \n",
    "        Args:\n",
    "            num_episodes: Number of episodes to train for\n",
    "        \"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            # Reset environment and get initial state\n",
    "            self.env.reset()\n",
    "            state = self.env.loc\n",
    "            \n",
    "            # Choose initial action\n",
    "            action = self.choose_action(state)\n",
    "            \n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            terminated = False\n",
    "            \n",
    "            while not terminated:\n",
    "                # Take action, observe reward and next state\n",
    "                next_state, reward, terminated = self.env.step(action)\n",
    "\n",
    "                # Negative reward (we want to minimize penalties)\n",
    "                reward = -reward  \n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                if not terminated:\n",
    "                    # Choose next action (SARSA is on-policy)\n",
    "                    next_action = self.choose_action(next_state)\n",
    "                    \n",
    "                    # Update Q-value using SARSA update rule\n",
    "                    x, y = state\n",
    "                    nx, ny = next_state\n",
    "                    \n",
    "                    # Q(s,a) = Q(s,a) + alpha * [r + gamma * Q(s',a') - Q(s,a)]\n",
    "                    self.Q[x, y, action] += self.alpha * (\n",
    "                        reward + self.gamma * self.Q[nx, ny, next_action] - self.Q[x, y, action]\n",
    "                    )\n",
    "                    \n",
    "                    # Move to next state and action\n",
    "                    state = next_state\n",
    "                    action = next_action\n",
    "                else:\n",
    "                    # Terminal state update\n",
    "                    x, y = state\n",
    "                    self.Q[x, y, action] += self.alpha * (reward - self.Q[x, y, action])\n",
    "            \n",
    "            # Track progress\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(steps)\n",
    "            \n",
    "            # Optionally reduce epsilon over time\n",
    "            # self.epsilon = max(0.01, self.epsilon * 0.995)\n",
    "            \n",
    "            if verbose and (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode+1}/{num_episodes}, Avg Reward: {np.mean(self.episode_rewards[-100:]):.2f}\")\n",
    "            elif episode == num_episodes - 1:\n",
    "                print(f\"Episode {episode+1}/{num_episodes}, Avg Reward: {np.mean(self.episode_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = CliffWalkingEnvironment()\n",
    "sarsa_agent = SARSAAgent(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent.train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent.train(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent.train(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent.train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Simulation and visualization code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run(agent, render=False):\n",
    "    \"\"\"\n",
    "    Test the trained agent on one episode.\n",
    "    \n",
    "    Args:\n",
    "        render: Whether to visualize the episode\n",
    "    \n",
    "    Returns:\n",
    "        Total reward and steps taken\n",
    "    \"\"\"\n",
    "    agent.env.reset()\n",
    "    state = agent.env.loc\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    terminated = False\n",
    "    \n",
    "    path = [state]  # Track the agent's path\n",
    "    \n",
    "    while not terminated:\n",
    "        # Choose action (no exploration)\n",
    "        action = agent.choose_action(state, explore=False)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, terminated = agent.env.step(action)\n",
    "        reward = -reward  # Convert penalty to reward\n",
    "        \n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        state = next_state\n",
    "        \n",
    "        path.append(state)  # Add to path\n",
    "        \n",
    "    return path, total_reward, steps\n",
    "\n",
    "def visualize_policy(agent, path=None):\n",
    "    \"\"\"\n",
    "    Visualize the learned policy and the environment.\n",
    "    \n",
    "    Args:\n",
    "        path: Optional list of states visited during an episode\n",
    "    \"\"\"\n",
    "    policy_map = np.zeros((agent.env.height, agent.env.width), dtype=int)\n",
    "    value_map = np.zeros((agent.env.height, agent.env.width))\n",
    "    \n",
    "    # Create policy and value maps\n",
    "    for y in range(agent.env.height):\n",
    "        for x in range(agent.env.width):\n",
    "            state = (x, y)\n",
    "            valid_actions = agent.env.enumerate_options(state)\n",
    "            if valid_actions:\n",
    "                q_values = [agent.Q[x, y, a] if a in valid_actions else -np.inf for a in range(4)]\n",
    "                policy_map[agent.env.height - 1 - y, x] = np.argmax(q_values)\n",
    "                value_map[agent.env.height - 1 - y, x] = max([q_values[a] for a in valid_actions])\n",
    "    \n",
    "    # Create a grid\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    # Define colors for different cell types\n",
    "    colors = ['white', 'lightgrey', 'lightgreen', 'red']\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Create grid\n",
    "    grid = np.zeros((agent.env.height, agent.env.width))\n",
    "    \n",
    "    # Mark cliff cells\n",
    "    for x, y in agent.env.cliff:\n",
    "        grid[agent.env.height - 1 - y, x] = 3  # Red\n",
    "    \n",
    "    # Mark start cell\n",
    "    start_x, start_y = agent.env.start_loc\n",
    "    grid[agent.env.height - 1 - start_y, start_x] = 1  # Light grey\n",
    "    \n",
    "    # Mark goal cell\n",
    "    goal_x, goal_y = agent.env.end_loc\n",
    "    grid[agent.env.height - 1 - goal_y, goal_x] = 2  # Light green\n",
    "    \n",
    "    # Plot grid\n",
    "    ax.imshow(grid, cmap=cmap)\n",
    "    \n",
    "    # Plot policy arrows\n",
    "    for y in range(agent.env.height):\n",
    "        for x in range(agent.env.width):\n",
    "            if grid[y, x] != 3:  # Skip cliff cells\n",
    "                action = policy_map[y, x]\n",
    "                if action == 0:  # Left\n",
    "                    ax.arrow(x, y, -0.3, 0, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "                elif action == 1:  # Right\n",
    "                    ax.arrow(x, y, 0.3, 0, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "                elif action == 2:  # Up\n",
    "                    ax.arrow(x, y, 0, -0.3, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "                elif action == 3:  # Down\n",
    "                    ax.arrow(x, y, 0, 0.3, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Plot path if provided\n",
    "    if path:\n",
    "        path_x = [x for x, y in path]\n",
    "        path_y = [agent.env.height - 1 - y for x, y in path]\n",
    "        ax.plot(path_x, path_y, 'b-', linewidth=2, alpha=0.5)\n",
    "        ax.plot(path_x[0], path_y[0], 'go', markersize=10)  # Start\n",
    "        ax.plot(path_x[-1], path_y[-1], 'ro', markersize=10)  # End\n",
    "    \n",
    "    # Add grid lines\n",
    "    ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=1)\n",
    "    ax.set_xticks(np.arange(-0.5, agent.env.width, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, agent.env.height, 1), minor=True)\n",
    "    ax.grid(which='minor', color='k', linestyle='-', linewidth=1)\n",
    "    ax.set_xticks(np.arange(0, agent.env.width, 1))\n",
    "    ax.set_yticks(np.arange(0, agent.env.height, 1))\n",
    "    \n",
    "    # Create legend\n",
    "    start_patch = mpatches.Patch(color='lightgrey', label='Start')\n",
    "    goal_patch = mpatches.Patch(color='lightgreen', label='Goal')\n",
    "    cliff_patch = mpatches.Patch(color='red', label='Cliff')\n",
    "    path_line = plt.Line2D([0], [0], color='blue', linewidth=2, alpha=0.5, label='Path')\n",
    "    \n",
    "    ax.legend(handles=[start_patch, goal_patch, cliff_patch, path_line], \n",
    "              loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "    \n",
    "    plt.title('SARSA Policy for Cliff Walking')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, penalty, steps = test_run(sarsa_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy(sarsa_agent, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm\n",
    "\n",
    "Sarsa is known as an \"on-policy\" learning method because it chooses actions and updates from the same $Q$ function.\n",
    "\n",
    "Q-learning is what is known as an \"off-policy\" learning method because it will have the ability to use a different policy for proposing new actions but use the greedy policy when computing the temporal difference. This allows the algorithm to make use of $S, A, R, S'$ transitions obtained from *any* source, and still learn an approximation $Q$ that converges to $q^*$ with probability 1... Convergence requires some conditions, most importantly that the transitions $S, A, R, S'$ *cover* the action space of $q^*$\n",
    "\n",
    "Self-driving is a nice example of how these two strategies can differ:\n",
    "\n",
    "- Sarsa method:\n",
    "    - Give control of vehicle over to Sarsa, so it can choose $A$ and observe implied $R$, $S'$ transitions\n",
    "- Off-policy:\n",
    "    - Let human expert driver drive vehicle in intended way\n",
    "    - Record $S, A, R, S'$ transitions visited by human driver\n",
    "    - Train RL agent based on data generated from human experience\n",
    "\n",
    "![q-learning_barto_sutton.png](https://compsosci-resources.s3.amazonaws.com/images/q-learning_barto_sutton.png)\n",
    "\n",
    "The main difference is in the update step where there's an explicit max operation in the update equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize a Q-learning agent.\n",
    "        \n",
    "        Args:\n",
    "            env: The environment the agent will interact with\n",
    "            alpha: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon: Exploration rate\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        # State is (x, y) and actions are 0, 1, 2, 3 (left, right, up, down)\n",
    "        self.Q = np.zeros((env.width, env.height, 4))\n",
    "        \n",
    "        # For tracking learning progress\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "    \n",
    "    def choose_action(self, state, explore=True):\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (x, y)\n",
    "            explore: Whether to use exploration or not\n",
    "            \n",
    "        Returns:\n",
    "            The chosen action\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        \n",
    "        # Get valid actions from the environment\n",
    "        valid_actions = self.env.enumerate_options(state)\n",
    "        \n",
    "        if explore and random.random() < self.epsilon:\n",
    "            # Exploration: choose a random valid action\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            # Exploitation: choose the best action (with random tie-breaking)\n",
    "            q_values = [self.Q[x, y, a] if a in valid_actions else -np.inf for a in range(4)]\n",
    "            max_q = max([q_values[a] for a in valid_actions])\n",
    "            best_actions = [a for a in valid_actions if q_values[a] == max_q]\n",
    "            action = random.choice(best_actions)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def train(self, num_episodes=500, verbose=False):\n",
    "        \"\"\"\n",
    "        Train the agent using Q-learning algorithm.\n",
    "        \n",
    "        Args:\n",
    "            num_episodes: Number of episodes to train for\n",
    "        \"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            # Reset environment and get initial state\n",
    "            self.env.reset()\n",
    "            state = self.env.loc\n",
    "            \n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            terminated = False\n",
    "            \n",
    "            while not terminated:\n",
    "                # Choose action using epsilon-greedy policy\n",
    "                action = self.choose_action(state)\n",
    "                \n",
    "                # Take action, observe reward and next state\n",
    "                next_state, reward, terminated = self.env.step(action)\n",
    "                # Negative reward (we want to minimize penalties)\n",
    "                reward = -reward  \n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                # Update Q-value using Q-learning update rule\n",
    "                x, y = state\n",
    "                nx, ny = next_state\n",
    "                \n",
    "                if not terminated:\n",
    "                    # Q(s,a) = Q(s,a) + alpha * [r + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "                    # Get valid actions for next state\n",
    "                    next_valid_actions = self.env.enumerate_options(next_state)\n",
    "                    # Find maximum Q-value for next state\n",
    "                    next_q_values = [self.Q[nx, ny, a] if a in next_valid_actions else -np.inf for a in range(4)]\n",
    "                    max_next_q = max([next_q_values[a] for a in next_valid_actions])\n",
    "                    \n",
    "                    # Update current Q-value\n",
    "                    self.Q[x, y, action] += self.alpha * (\n",
    "                        reward + self.gamma * max_next_q - self.Q[x, y, action]\n",
    "                    )\n",
    "                else:\n",
    "                    # Terminal state update (no future rewards)\n",
    "                    self.Q[x, y, action] += self.alpha * (reward - self.Q[x, y, action])\n",
    "                \n",
    "                # Move to next state\n",
    "                state = next_state\n",
    "            \n",
    "            # Track progress\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(steps)\n",
    "            \n",
    "            # Optionally reduce epsilon over time for better exploitation\n",
    "            # self.epsilon = max(0.01, self.epsilon * 0.995)\n",
    "            \n",
    "            if verbose and (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode+1}/{num_episodes}, Avg Reward: {np.mean(self.episode_rewards[-100:]):.2f}\")\n",
    "            elif episode == num_episodes - 1:\n",
    "                print(f\"Episode {episode+1}/{num_episodes}, Avg Reward: {np.mean(self.episode_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = CliffWalkingEnvironment()\n",
    "q_agent = QLearningAgent(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.train(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.train(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Simulation and visualization code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, penalty, steps = test_run(q_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy(q_agent, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
